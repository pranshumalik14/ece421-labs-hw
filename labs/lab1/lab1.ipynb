{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1A_tNKGCLmCrQZGujHuIZhwkEe1mxQJHo",
      "authorship_tag": "ABX9TyMQfyjQNeDSE7zbzjtzN9rb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranshumalik14/ece421-labs-hw/blob/main/labs/lab1/lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 1: Logistic Regression\n",
        "\n",
        "In this lab, we will be creating a binary classifier using Logistic Regression first implemented using Numpy and then using Tensorflow. The classifier has to be trained on the `notMNIST` dataset, and particularly classify only between the letters `C` (positive class, labelled `1`) and `J` (negative class, labelled `0`). This dataset, representing the ground truth, will be represented as $\\mathcal{D}= \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N$, where there are $N$ datavectors $\\mathbf{x}_i \\in \\mathbb{R}^d$ and labels $y_i \\in \\{0, 1\\}$."
      ],
      "metadata": {
        "id": "4nyBLeZpQvLi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Logistic Regression with Numpy\n",
        "\n",
        "We use the following model for computing the probability of a datavector $\\mathbf{x}_n\\in\\mathbb{R}^d$ belonging to a particular class $y_n\\in \\{0, 1\\}$:\n",
        "\n",
        "$$\\hat{p}_\\mathbf{w}(y_n\\mid \\mathbf{x}_n) = \\sigma\\left((2y_n-1)(\\mathbf{w}^\\top\\mathbf{x}_n + b)\\right),$$\n",
        "\n",
        "given the model parameters $\\mathbf{w} \\in \\mathbb{R}^d$ (wieght) and $b \\in \\mathbb{R}$ (bias), and the logistic (or sigmoid) function $\\sigma(z) = \\frac{1}{1+e^{-z}}$.\n",
        "\n",
        "However, to simply expressions we will augment our datavectors $\\mathbf{x}_i$ with ones and accordingly increment the dimension of the weightvector, such that $\\mathbf{x}_i = \\begin{bmatrix}1 & x_1 & \\ldots & x_d\\end{bmatrix} \\in \\mathbb{R}^{d+1}$ and $\\mathbf{w} = \\begin{bmatrix}b & w_1 & \\ldots & w_d \\end{bmatrix}\\in \\mathbb{R}^{d+1}$. This will yield the (binary) label prediction probability to be:\n",
        "\n",
        "$$\\hat{p}_\\mathbf{w}(y_n\\mid \\mathbf{x}_n) = \\sigma\\left((2y_n-1)(\\mathbf{w}^\\top\\mathbf{x}_n)\\right).$$\n",
        "No changes are made to the label sequence $y_i$."
      ],
      "metadata": {
        "id": "tIcxM0u-csSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "Iz52X8Wu_D4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Loss Function and Gradient \n",
        "\n",
        "We will use the regularized loss function (in-sample error) for minimization while training over the dataset:\n",
        "\n",
        "$$\\begin{align*}\n",
        "E_{\\text{in}, \\lambda}(\\mathbf{w}) &= \\lambda||\\mathbf{w}||^2 + \\frac{1}{N}\\sum_{n=1}^N -\\log\\left(\\hat{p}_\\mathbf{w}(y_n\\mid \\mathbf{x}_n)\\right)\\\\\n",
        "&= \\lambda||\\mathbf{w}||^2 + \\frac{1}{N}\\sum_{n=1}^N \\big[-I(y_n=1)\\log(\\hat{p}_\\mathbf{w}(1\\mid \\mathbf{x}_n)) -I(y_n=0)\\log(\\hat{p}_\\mathbf{w}(0\\mid \\mathbf{x}_n))\\big] \\quad \\triangleright \\text{since } y_n \\text{ only has two possibilities}\\\\\n",
        "&= \\lambda||\\mathbf{w}||^2 + \\frac{1}{N}\\sum_{n=1}^N \\big[-y_n\\log(\\hat{p}_\\mathbf{w}(1\\mid \\mathbf{x}_n)) -(1-y_n)\\log(1-\\hat{p}_\\mathbf{w}(1\\mid \\mathbf{x}_n))\\big],\n",
        "\\end{align*}\n",
        "$$\n",
        "where $\\lambda > 0$ is the regularization constant and $I(p)$ is the identifier function defined to be, $I(p) = \\begin{cases}1 & \\text{predicate } p \\text{ is true}\\\\ 0 & \\text{predicate } p \\text{ is false}\\end{cases}$\n",
        "\n",
        "The gradient of the loss function is:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "∇_{\\mathbf{w}}E_{\\text{in}, \\lambda}(\\mathbf{w}) &= ∇_{\\mathbf{w}}\\lambda||\\mathbf{w}||^2 + \\frac{1}{N}\\sum_{n=1}^N ∇_{\\mathbf{w}}\\big[-y_n\\log(\\hat{p}_\\mathbf{w}(1\\mid \\mathbf{x}_n)) -(1-y_n)\\log(1-\\hat{p}_\\mathbf{w}(1\\mid \\mathbf{x}_n))\\big]\\\\\n",
        "&=2\\lambda\\mathbf{w} + \\frac{1}{N}\\sum_{n=1}^N\\big[-y_n(1+e^{-\\mathbf{w}^\\top\\mathbf{x}_n})∇_{\\mathbf{w}}(1+e^{-\\mathbf{w}^\\top\\mathbf{x}_n})^{-1}-(1-y_n)(1+e^{\\mathbf{w}^\\top\\mathbf{x}_n})∇_{\\mathbf{w}}(1+e^{\\mathbf{w}^\\top\\mathbf{x}_n})^{-1}\\big]\\\\\n",
        "&=2\\lambda\\mathbf{w} + \\frac{1}{N}\\sum_{n=1}^N\\big[-y_n\\mathbf{x}_n\\underbrace{\\frac{e^{-\\mathbf{w}^\\top\\mathbf{x}_n}}{(1+e^{-\\mathbf{w}^\\top\\mathbf{x}_n})}}_{\\sigma(-\\mathbf{w}^\\top\\mathbf{x}_n) = 1-\\sigma(\\mathbf{w}^\\top\\mathbf{x}_n)}+(1-y_n)\\mathbf{x}_n\\underbrace{\\frac{e^{\\mathbf{w}^\\top\\mathbf{x}_n}}{(1+e^{\\mathbf{w}^\\top\\mathbf{x}_n})}}_{\\sigma(\\mathbf{w}^\\top\\mathbf{x}_n)}\\big]\\\\\n",
        "&=2\\lambda\\mathbf{w} + \\frac{1}{N}\\sum_{n=1}^N \\mathbf{x}_n\\left(\\sigma(\\mathbf{w}^\\top\\mathbf{x}_n)-y_n\\right)\\\\\n",
        "&=2\\lambda\\mathbf{w} + \\frac{1}{N}\\mathbf{X}^\\top\\left(\\boldsymbol{\\sigma}(\\mathbf{X}\\mathbf{w})-\\mathbf{y}\\right), \n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "where $\\mathbf{X} = \\begin{bmatrix}\\mathbf{x}_1^\\top\\\\\\vdots\\\\\\mathbf{x}_N^\\top\\end{bmatrix}\\in \\mathbb{R}^{N\\times(d+1)}$ is the datamatrix, $\\mathbf{y} \\in \\{0,1\\}^{N}$ is the labelvector, and $\\boldsymbol{\\sigma}(\\cdot)$ is a vector function acting element-wise on the input vector by applying the (scalar) logistic function."
      ],
      "metadata": {
        "id": "KgmTrGuPkxCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(w, b, X, ys, reg_lambda):\n",
        "    sigmoid = lambda z: 1 / (1 + np.exp(-z))\n",
        "    \n",
        "    return sigmoid(X@w + b)"
      ],
      "metadata": {
        "id": "6B3qA2QDQ8h8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_loss(w, b, X, ys, reg_lambda):\n",
        "    return 1.0"
      ],
      "metadata": {
        "id": "BB2nSCFZtaCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Gradient Descent Implementation\n",
        "\n",
        "The `notMNIST` dataset has images of size $28\\times 28$, thus we have $d=784$. "
      ],
      "metadata": {
        "id": "se9KDMkrv42r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# grad descent"
      ],
      "metadata": {
        "id": "iEitGSOafDDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will test it on the data."
      ],
      "metadata": {
        "id": "PsrzemJdfE0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loadDataGDrive():\n",
        "    with np.load('/content/drive/MyDrive/Colab Notebooks/notMNIST.npz') as dataset:\n",
        "        Data, Target = dataset['images'], dataset['labels']\n",
        "        posClass = 2\n",
        "        negClass = 9\n",
        "        dataIndx = (Target==posClass) + (Target==negClass)\n",
        "        Data = Data[dataIndx]/255.\n",
        "        Target = Target[dataIndx].reshape(-1, 1)\n",
        "        Target[Target==posClass] = 1\n",
        "        Target[Target==negClass] = 0\n",
        "        np.random.seed(421)\n",
        "        randIndx = np.arange(len(Data))\n",
        "        np.random.shuffle(randIndx)\n",
        "        Data, Target = Data[randIndx], Target[randIndx]\n",
        "        trainData, trainTarget = Data[:3500], Target[:3500]\n",
        "        validData, validTarget = Data[3500:3600], Target[3500:3600]\n",
        "        testData, testTarget = Data[3600:], Target[3600:]\n",
        "    return trainData, validData, testData, trainTarget, validTarget, testTarget\n",
        "\n",
        "x_train, x_valid, x_test, y_train, y_valid, y_test = loadDataGDrive()"
      ],
      "metadata": {
        "id": "kSQHXyZX_f8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# augment data"
      ],
      "metadata": {
        "id": "UhyvCbSae-2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run grad_desc (output optimal things)"
      ],
      "metadata": {
        "id": "DvIkYG2RfJeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Tuning the Learning Rate"
      ],
      "metadata": {
        "id": "B5kN_yGowGdp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Generalization\n"
      ],
      "metadata": {
        "id": "Rlixb3nBwLR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kfwX31nhwUDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Logistic Regression in TensorFlow\n"
      ],
      "metadata": {
        "id": "FS3R6sO3wVOm"
      }
    }
  ]
}