{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1A_tNKGCLmCrQZGujHuIZhwkEe1mxQJHo",
      "authorship_tag": "ABX9TyMnlnlq4nT7Jenzae8ZPNFb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranshumalik14/ece421-labs-hw/blob/main/labs/lab1/lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 1: Logistic Regression\n",
        "\n",
        "In this lab, we will be creating a binary classifier using Logistic Regression first implemented using Numpy and then using Tensorflow. The classifier has to be trained on the `notMNIST` dataset, and particularly classify only between the letters `C` (positive class, labelled `1`) and `J` (negative class, labelled `0`). This dataset, representing the ground truth, will be represented as $\\mathcal{D}= \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N$, where there are $N$ datavectors $\\mathbf{x}_i \\in \\mathbb{R}^d$ and labels $y_i \\in \\{0, 1\\}$."
      ],
      "metadata": {
        "id": "4nyBLeZpQvLi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Logistic Regression with Numpy\n",
        "\n",
        "We use the following model for computing the probability of a datavector $\\mathbf{x}_n\\in\\mathbb{R}^d$ belonging to a particular class $y_n\\in \\{0, 1\\}$:\n",
        "\n",
        "$$\\hat{p}_\\mathbf{w}(y_n\\mid \\mathbf{x}_n) = \\sigma\\left((2y_n-1)(\\mathbf{w}^\\top\\mathbf{x}_n + b)\\right),$$\n",
        "\n",
        "given the model parameters $\\mathbf{w} \\in \\mathbb{R}^d$ (wieght) and $b \\in \\mathbb{R}$ (bias), and the logistic (or sigmoid) function $\\sigma(z) = \\frac{1}{1+e^{-z}}$.\n",
        "\n",
        "However, to simply expressions we will augment our datavectors $\\mathbf{x}_i$ with ones and accordingly increment the dimension of the weightvector, such that $\\mathbf{x}_i = \\begin{bmatrix}1 & x_1 & \\ldots & x_d\\end{bmatrix} \\in \\mathbb{R}^{d+1}$ and $\\mathbf{w} = \\begin{bmatrix}b & w_1 & \\ldots & w_d \\end{bmatrix}\\in \\mathbb{R}^{d+1}$. This will yield the (binary) label prediction probability to be:\n",
        "\n",
        "$$\\hat{p}_\\mathbf{w}(y_n\\mid \\mathbf{x}_n) = \\sigma\\left((2y_n-1)(\\mathbf{w}^\\top\\mathbf{x}_n)\\right).$$\n",
        "No changes are made to the label sequence $y_i$. Also note that the corresponding changes to function signatures were made, so the functions defined in this document (e.g. `loss` and `grad_loss`) will differ slightly from the lab handout."
      ],
      "metadata": {
        "id": "tIcxM0u-csSX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Loss Function and Gradient \n",
        "\n",
        "We will use the regularized loss function (in-sample error) for minimization while training over the dataset:\n",
        "\n",
        "$$\\begin{align*}\n",
        "E_{\\text{in}, \\lambda}(\\mathbf{w}) &= \\lambda||\\mathbf{w}||^2 + \\frac{1}{N}\\sum_{n=1}^N -\\log\\left(\\hat{p}_\\mathbf{w}(y_n\\mid \\mathbf{x}_n)\\right)\\\\\n",
        "&= \\lambda||\\mathbf{w}||^2 + \\frac{1}{N}\\sum_{n=1}^N \\big[-I(y_n=1)\\log(\\hat{p}_\\mathbf{w}(1\\mid \\mathbf{x}_n)) -I(y_n=0)\\log(\\hat{p}_\\mathbf{w}(0\\mid \\mathbf{x}_n))\\big] \\quad \\triangleright \\text{since } y_n \\text{ only has two possibilities}\\\\\n",
        "&= \\lambda||\\mathbf{w}||^2 + \\frac{1}{N}\\sum_{n=1}^N \\big[-y_n\\log(\\hat{p}_\\mathbf{w}(1\\mid \\mathbf{x}_n)) -(1-y_n)\\log(1-\\hat{p}_\\mathbf{w}(1\\mid \\mathbf{x}_n))\\big]\\\\\n",
        "&= \\lambda||\\mathbf{w}||^2 + \\frac{1}{N}\\sum_{n=1}^N \\big[y_n\\log(1 + e^{-\\mathbf{w}^\\top\\mathbf{x}_n})) +(1-y_n)\\log(1+e^{\\mathbf{w}^\\top\\mathbf{x}_n})\\big],\n",
        "\\end{align*}\n",
        "$$\n",
        "where $\\lambda > 0$ is the regularization constant and $I(p)$ is the identifier function defined to be, $I(p) = \\begin{cases}1 & \\text{predicate } p \\text{ is true}\\\\ 0 & \\text{predicate } p \\text{ is false}\\end{cases}$\n",
        "\n",
        "The gradient of the loss function is:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "∇_{\\mathbf{w}}E_{\\text{in}, \\lambda}(\\mathbf{w}) &= ∇_{\\mathbf{w}}\\lambda||\\mathbf{w}||^2 + \\frac{1}{N}\\sum_{n=1}^N ∇_{\\mathbf{w}}\\big[-y_n\\log(\\hat{p}_\\mathbf{w}(1\\mid \\mathbf{x}_n)) -(1-y_n)\\log(1-\\hat{p}_\\mathbf{w}(1\\mid \\mathbf{x}_n))\\big]\\\\\n",
        "&=2\\lambda\\mathbf{w} + \\frac{1}{N}\\sum_{n=1}^N\\big[-y_n(1+e^{-\\mathbf{w}^\\top\\mathbf{x}_n})∇_{\\mathbf{w}}(1+e^{-\\mathbf{w}^\\top\\mathbf{x}_n})^{-1}-(1-y_n)(1+e^{\\mathbf{w}^\\top\\mathbf{x}_n})∇_{\\mathbf{w}}(1+e^{\\mathbf{w}^\\top\\mathbf{x}_n})^{-1}\\big]\\\\\n",
        "&=2\\lambda\\mathbf{w} + \\frac{1}{N}\\sum_{n=1}^N\\big[-y_n\\mathbf{x}_n\\underbrace{\\frac{e^{-\\mathbf{w}^\\top\\mathbf{x}_n}}{(1+e^{-\\mathbf{w}^\\top\\mathbf{x}_n})}}_{\\sigma(-\\mathbf{w}^\\top\\mathbf{x}_n) = 1-\\sigma(\\mathbf{w}^\\top\\mathbf{x}_n)}+(1-y_n)\\mathbf{x}_n\\underbrace{\\frac{e^{\\mathbf{w}^\\top\\mathbf{x}_n}}{(1+e^{\\mathbf{w}^\\top\\mathbf{x}_n})}}_{\\sigma(\\mathbf{w}^\\top\\mathbf{x}_n)}\\big]\\\\\n",
        "&=2\\lambda\\mathbf{w} + \\frac{1}{N}\\sum_{n=1}^N \\mathbf{x}_n\\left(\\sigma(\\mathbf{w}^\\top\\mathbf{x}_n)-y_n\\right)\\\\\n",
        "&=2\\lambda\\mathbf{w} + \\frac{1}{N}\\mathbf{X}^\\top\\left(\\boldsymbol{\\sigma}(\\mathbf{X}\\mathbf{w})-\\mathbf{y}\\right), \n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "where $\\mathbf{X} = \\begin{bmatrix}\\mathbf{x}_1^\\top\\\\\\vdots\\\\\\mathbf{x}_N^\\top\\end{bmatrix}\\in \\mathbb{R}^{N\\times(d+1)}$ is the datamatrix, $\\mathbf{y} \\in \\{0,1\\}^{N}$ is the labelvector, and $\\boldsymbol{\\sigma}(\\cdot)$ is a vector function acting element-wise on the input vector by applying the (scalar) logistic function."
      ],
      "metadata": {
        "id": "KgmTrGuPkxCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Iz52X8Wu_D4Z"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sigmoid = lambda z: 1 / (1 + np.exp(-z))"
      ],
      "metadata": {
        "id": "cehSNA51nnQt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(w, X, ys, reg_lambda):\n",
        "    Xw = X @ w\n",
        "    logit_CE_loss = ys*np.log(1 + np.exp(-Xw)) + (1 - ys)*np.log(1 + np.exp(Xw))\n",
        "    # return regularization penalty + cross entropy loss\n",
        "    return reg_lambda*np.linalg.norm(w)**2 + np.mean(logit_CE_loss)"
      ],
      "metadata": {
        "id": "6B3qA2QDQ8h8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_loss(w, X, ys, reg_lambda):\n",
        "    return 2*reg_lambda*w + 1/X.shape[0]*(X.T @ (sigmoid(X@w) - ys))"
      ],
      "metadata": {
        "id": "BB2nSCFZtaCQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Gradient Descent Implementation\n",
        "\n",
        "The update step is:\n",
        "\n",
        "$$\\mathbf{w}_{k+1} = \\mathbf{w}_k - ϵ\\underbrace{∇_{\\mathbf{w}}E_{\\text{in}, λ}(\\mathbf{w}_k)}_{\\texttt{grad_loss}(\\mathbf{w}_k)},$$\n",
        "\n",
        "where $ϵ > 0$ is the learning rate."
      ],
      "metadata": {
        "id": "se9KDMkrv42r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_descent(w, X, ys, eps, epochs, reg_lambda, error_tol=1e-7):\n",
        "    w_opt     = np.copy(w)\n",
        "    loss_hist = []\n",
        "    for i in range(epochs):\n",
        "        grad_w = grad_loss(w, X, ys, reg_lambda)\n",
        "        w_opt  = w - eps*grad_w\n",
        "        if np.linalg.norm(w-w_opt)**2 < error_tol:\n",
        "            break\n",
        "        else:\n",
        "            w = w_opt\n",
        "        loss_hist.append(loss(w, X, ys, reg_lambda))\n",
        "    return w_opt, loss_hist"
      ],
      "metadata": {
        "id": "iEitGSOafDDt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will test it on the data. The `notMNIST` dataset has images of size $28\\times 28$, thus we have $d=784$. "
      ],
      "metadata": {
        "id": "PsrzemJdfE0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "def loadDataGDrive():\n",
        "    with np.load('/content/drive/MyDrive/Colab Notebooks/notMNIST.npz') as dataset:\n",
        "        Data, Target = dataset['images'], dataset['labels']\n",
        "        posClass = 2\n",
        "        negClass = 9\n",
        "        dataIndx = (Target==posClass) + (Target==negClass)\n",
        "        Data = Data[dataIndx]/255.\n",
        "        Target = Target[dataIndx].reshape(-1, 1)\n",
        "        Target[Target==posClass] = 1\n",
        "        Target[Target==negClass] = 0\n",
        "        np.random.seed(421)\n",
        "        randIndx = np.arange(len(Data))\n",
        "        np.random.shuffle(randIndx)\n",
        "        Data, Target = Data[randIndx], Target[randIndx]\n",
        "        trainData, trainTarget = Data[:3500], Target[:3500]\n",
        "        validData, validTarget = Data[3500:3600], Target[3500:3600]\n",
        "        testData, testTarget = Data[3600:], Target[3600:]\n",
        "    return trainData, validData, testData, trainTarget, validTarget, testTarget\n",
        "\n",
        "x_train, x_valid, x_test, ys_train, ys_valid, ys_test = loadDataGDrive()"
      ],
      "metadata": {
        "id": "kSQHXyZX_f8R"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# augment datavectors\n",
        "X_train = np.concatenate([np.ones((x_train.shape[0])).reshape(-1, 1), x_train.reshape(x_train.shape[0], -1)], axis=1)\n",
        "X_valid = np.concatenate([np.ones((x_valid.shape[0])).reshape(-1, 1), x_valid.reshape(x_valid.shape[0], -1)], axis=1)\n",
        "X_test  = np.concatenate([np.ones((x_test.shape[0])).reshape(-1, 1), x_test.reshape(x_test.shape[0], -1)], axis=1)"
      ],
      "metadata": {
        "id": "Npi0SUmwopC3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run grad_desc (output optimal things)\n",
        "w_init = np.random.normal(0.001, 0.99, (X_train.shape[1], 1))\n",
        "w_star, loss_history = grad_descent(w_init, X_train, ys_train, 0.005, 10000, 0)"
      ],
      "metadata": {
        "id": "DvIkYG2RfJeA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "ciBhN5Pjohcm",
        "outputId": "19f84886-f373-40ea-fca0-e2501e7ac8e2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f20148c2890>]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Tuning the Learning Rate"
      ],
      "metadata": {
        "id": "B5kN_yGowGdp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Generalization\n"
      ],
      "metadata": {
        "id": "Rlixb3nBwLR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kfwX31nhwUDj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Logistic Regression in TensorFlow\n"
      ],
      "metadata": {
        "id": "FS3R6sO3wVOm"
      }
    }
  ]
}