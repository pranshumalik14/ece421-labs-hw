{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1yAohXBUxBSMcoMJD6jAZ4wnN7GDJ869G",
      "authorship_tag": "ABX9TyNA7gfCgY3x3OnXgIZCabWp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranshumalik14/ece421-labs-hw/blob/main/labs/lab2/lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 2: Neural Networks\n",
        "\n",
        "In this lab, we will be creating a ."
      ],
      "metadata": {
        "id": "YtLiBTx7KvEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Neural Networks using Numpy\n",
        "\n",
        "We implement.\n"
      ],
      "metadata": {
        "id": "NBl4puliM4oc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Helper Functions, Network Definitions, and Initialization"
      ],
      "metadata": {
        "id": "jGWsXc7S6reh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Gff6Ah1DL3Oy"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first start with defining the activation functions along with their derivatives, which we will later be able to use in our neural networks."
      ],
      "metadata": {
        "id": "_P4168HVCneW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# activation functions\n",
        "class ReLU:\n",
        "    # relu = max(x, 0)    \n",
        "    @staticmethod\n",
        "    def __call__(x):\n",
        "        return np.maximum(x, 0)\n",
        "    # derivative of relu\n",
        "    @staticmethod\n",
        "    def prime(x):\n",
        "        return np.where(x > 0.0, 1.0, 0.0)\n",
        "\n",
        "class Softmax:\n",
        "    # softmax = exp(xi)/sum(exp(xj)), applied row-wise to match data\n",
        "    @staticmethod\n",
        "    def __call__(x):\n",
        "        # subtract maximum for numerical stability\n",
        "        # while keeping the relative differences intact\n",
        "        x -= x.max(axis=1, keepdims=True)\n",
        "        return np.exp(x)/np.exp(x).sum(axis=1, keepdims=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def prime(x):\n",
        "        raise NotImplementedError # todo: understand how this will be used"
      ],
      "metadata": {
        "id": "nFjDwg6nCukr"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we define the `Network` class that will be used to define an arbitrary network with dense (fully-connected) layers. The class definition and functionality will expand as we progress through each section. We begin with defining the functions that create the network structure, model parameters, their initialization, and the forward pass through the network.\n",
        "\n",
        "<TODO: ðŸš§ theory behind NN structure, notation>\n",
        "\n",
        "<TODO: ðŸš§ add info about Xavier initialization>"
      ],
      "metadata": {
        "id": "7CajgVMRH1BU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network:\n",
        "    def __init__(self, layer_sizes, layer_activations):\n",
        "        assert len(layer_activations) == len(layer_sizes)-1\n",
        "        self.sizes   = layer_sizes\n",
        "        self.thetas  = layer_activations\n",
        "\n",
        "        # xavier init scheme\n",
        "        n_out = layer_sizes[2:] + [0]\n",
        "        n_in  = layer_sizes[:-1]\n",
        "        self.biases  = [np.zeros((1, n)) for n in layer_sizes[1:]] # row-vecs\n",
        "        self.weights = [np.random.randn(m,n)/np.sqrt((n_out[i]+n_in[i])/2) for \n",
        "                        i, (m, n) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:]))]\n",
        "    \n",
        "    def fwd_prop(self, X):\n",
        "        X_ell = X\n",
        "        for W, b, theta in zip(self.weights, self.biases, self.thetas):\n",
        "            X_ell = theta((X_ell @ W) + b)\n",
        "        return X_ell"
      ],
      "metadata": {
        "id": "i60AbZ7kM-SE"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now define the loss function we will use to train our model, along with the accuracy model <>... ðŸš§ðŸš§ðŸš§ðŸš§. Like lab 1, we will use the cross entropy loss since it maximum likelihood and mutually exlusive outcomes and is fit for the classification task at hand."
      ],
      "metadata": {
        "id": "TSNVfnB7Iwdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loss function\n",
        "class CrossEntropy:\n",
        "    @staticmethod\n",
        "    def __call__(p_hat, ys):\n",
        "        return -np.sum(ys*np.log(p_hat))\n",
        "\n",
        "    @staticmethod\n",
        "    def derivative(logits, target):\n",
        "        raise NotImplementedError # todo: check inputs and requirement"
      ],
      "metadata": {
        "id": "z28ycr7EPLEB"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also define the `accuracy` function to compute the mean number of misclassifications, giving us an idea of how likely it is for the network to arrive at an incorrect label."
      ],
      "metadata": {
        "id": "9EDfOQALziwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy of prediction\n",
        "def accuracy(p_hat, ys):\n",
        "    return np.mean(np.argmax(ys, axis=1) == np.argmax(p_hat, axis=1))"
      ],
      "metadata": {
        "id": "4a5RHYFSzh67"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will run a forward pass on an untrained, newly initialized, network and test for its accuracy to make sure our implementation is functional thus far. We source the data from section 1.3 just for this purpose."
      ],
      "metadata": {
        "id": "SaSIXe7x41F3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create neural network\n",
        "F, H, K = 784, 1000, 10\n",
        "nn = Network([F, H, K], [ReLU(), Softmax()])\n",
        "\n",
        "# forward pass, loss, and accuracy on random init\n",
        "p_hat    = nn.fwd_prop(X_train)\n",
        "accur    = accuracy(p_hat, ys_train)\n",
        "avg_loss = CrossEntropy()(p_hat, ys_train)/X_train.shape[0]\n",
        "print(\"Loss = {}, Accuracy = {}\".format(avg_loss, accur))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NC9XcHR45IcX",
        "outputId": "da880a7f-5054-4e39-b921-2c46bce63bc5"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss = 2.741738154007161, Accuracy = 0.1128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Backpropagation Derivation\n",
        "\n",
        "We derive the gradient of the loss function with respect to the model parameters, to be able to update them."
      ],
      "metadata": {
        "id": "bRSG-YED5RFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ztqBfrR863pf"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Learning\n",
        "\n",
        "Training and data"
      ],
      "metadata": {
        "id": "GeXxzhsBPAaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data loader and associated helper functions\n",
        "def loadDataGDrive():\n",
        "    with np.load('/content/drive/MyDrive/Colab Notebooks/notMNIST.npz') as data:\n",
        "        data, targets = data[\"images\"], data[\"labels\"]\n",
        "        \n",
        "        np.random.seed(521)\n",
        "        rand_idx = np.arange(len(data))\n",
        "        np.random.shuffle(rand_idx)\n",
        "        \n",
        "        data = data[rand_idx] / 255.0\n",
        "        targets = targets[rand_idx].astype(int)\n",
        "        \n",
        "        train_data, train_target = data[:10000], targets[:10000]\n",
        "        valid_data, valid_target = data[10000:16000], targets[10000:16000]\n",
        "        test_data, test_target = data[16000:], targets[16000:]\n",
        "        train_target, valid_target, test_target = convert_onehot(train_target, \n",
        "                                                                 valid_target, \n",
        "                                                                 test_target)\n",
        "    return train_data, valid_data, test_data, train_target, valid_target, test_target\n",
        "\n",
        "def convert_onehot(train_target, valid_target, test_target):\n",
        "    new_train = np.zeros((train_target.shape[0], 10))\n",
        "    new_valid = np.zeros((valid_target.shape[0], 10))\n",
        "    new_test = np.zeros((test_target.shape[0], 10))\n",
        "\n",
        "    for item in range(0, train_target.shape[0]):\n",
        "        new_train[item][train_target[item]] = 1\n",
        "    for item in range(0, valid_target.shape[0]):\n",
        "        new_valid[item][valid_target[item]] = 1\n",
        "    for item in range(0, test_target.shape[0]):\n",
        "        new_test[item][test_target[item]] = 1\n",
        "    return new_train, new_valid, new_test\n",
        "\n",
        "def shuffle(data, target):\n",
        "    np.random.seed(421)\n",
        "    rand_idx = np.random.permutation(len(data))\n",
        "    return data[rand_idx], target[rand_idx]\n",
        "\n",
        "# load data\n",
        "x_train, x_valid, x_test, ys_train, ys_valid, ys_test = loadDataGDrive()"
      ],
      "metadata": {
        "id": "USALvma_PC3q"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# flatten datavectors\n",
        "X_train = x_train.reshape(x_train.shape[0], -1)\n",
        "X_valid = x_valid.reshape(x_valid.shape[0], -1)\n",
        "X_test  = x_test.reshape(x_test.shape[0], -1)\n",
        "\n",
        "# data for training and testing our neural network\n",
        "Xs = np.array([X_train, X_valid, X_test], dtype=object)    # training, validation, and testing data\n",
        "Ys = np.array([ys_train, ys_valid, ys_test], dtype=object) # training, validation, and testing labels"
      ],
      "metadata": {
        "id": "4rmh5IYe9BEK"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_learning_history(title, loss_hist, acc_hist):\n",
        "    # loss_hist = [train_loss, valid_loss, test_loss]\n",
        "    # acc_hist  = [train_acc, valid_acc, test_acc]\n",
        "    f, ax  = plt.subplots(1, 2, figsize=(8,4))\n",
        "    labels = [\"Train\", \"Validation\", \"Test\"]\n",
        "\n",
        "    for i in range(loss_hist.shape[1]):\n",
        "        ax[0].plot(loss_hist[:, i], label=labels[i]) # plot loss history on left\n",
        "        ax[1].plot(acc_hist[:, i], label=labels[i])  # plot accuracy history on right\n",
        "    \n",
        "    # add lables and title\n",
        "    ax[0].set_xlabel(\"Epochs\")\n",
        "    ax[0].set_ylabel(\"Loss\")\n",
        "    ax[0].legend()\n",
        "    ax[1].set_xlabel(\"Epochs\")\n",
        "    ax[1].set_ylabel(\"Accuracy\")\n",
        "    ax[1].legend()\n",
        "    f.suptitle(title, fontsize=14, y=1.03)\n",
        "    return plt.tight_layout()"
      ],
      "metadata": {
        "id": "5te4wQeM-3g9"
      },
      "execution_count": 64,
      "outputs": []
    }
  ]
}