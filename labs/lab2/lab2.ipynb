{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1yAohXBUxBSMcoMJD6jAZ4wnN7GDJ869G",
      "authorship_tag": "ABX9TyNeAtND+16C1QFYxcJkjQfR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranshumalik14/ece421-labs-hw/blob/main/labs/lab2/lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 2: Neural Networks\n",
        "\n",
        "In this lab, we will be creating a ."
      ],
      "metadata": {
        "id": "YtLiBTx7KvEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Neural Networks using Numpy\n",
        "\n",
        "We implement.\n"
      ],
      "metadata": {
        "id": "NBl4puliM4oc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Helper Functions, Network Definitions, and Initialization"
      ],
      "metadata": {
        "id": "jGWsXc7S6reh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Gff6Ah1DL3Oy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first start with defining the activation functions along with their derivatives, which we will later be able to use in our neural networks."
      ],
      "metadata": {
        "id": "_P4168HVCneW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# activation functions\n",
        "class ReLU:\n",
        "    # relu = max(x, 0)    \n",
        "    @staticmethod\n",
        "    def __call__(x):\n",
        "        return np.maximum(x, 0)\n",
        "    # derivative of relu\n",
        "    @staticmethod\n",
        "    def prime(x):\n",
        "        return np.where(x > 0.0, 1.0, 0.0)\n",
        "\n",
        "class Softmax:\n",
        "    # softmax = exp(xi)/sum(exp(xj)), applied row-wise to match data\n",
        "    @staticmethod\n",
        "    def __call__(x):\n",
        "        # subtract maximum for numerical stability\n",
        "        # while keeping the relative differences intact\n",
        "        exp = np.exp(x - x.max(axis=1, keepdims=True))\n",
        "        return exp/exp.sum(axis=1, keepdims=True)\n",
        "\n",
        "    # we can not use this method for batch processing since it \n",
        "    # returns the full Jacobian and will be incompatible with\n",
        "    # element-wise products\n",
        "    @staticmethod\n",
        "    def prime(x):\n",
        "        assert \n",
        "        s = Softmax()(x)\n",
        "        return np.diag(s) - np.outer(s, s)"
      ],
      "metadata": {
        "id": "nFjDwg6nCukr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we define the `Network` class that will be used to define an arbitrary network with dense (fully-connected) layers. The class definition and functionality will expand as we progress through each section. We begin with defining the functions that create the network structure, model parameters, their initialization, and the forward pass through the network.\n",
        "\n",
        "<TODO: ðŸš§ theory behind NN structure, notation>\n",
        "\n",
        "<TODO: ðŸš§ add info about Xavier initialization>"
      ],
      "metadata": {
        "id": "7CajgVMRH1BU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network:\n",
        "    def __init__(self, layer_sizes, layer_activations):\n",
        "        assert len(layer_activations) == len(layer_sizes)-1\n",
        "        self.sizes   = layer_sizes\n",
        "        self.thetas  = layer_activations\n",
        "\n",
        "        # xavier init scheme\n",
        "        n_out = layer_sizes[2:] + [0]\n",
        "        n_in  = layer_sizes[:-1]\n",
        "        self.biases  = [np.zeros((1, n)) for n in layer_sizes[1:]] # row-vecs\n",
        "        self.weights = [np.random.randn(m,n)/np.sqrt((n_out[i]+n_in[i])/2) for \n",
        "                        i, (m, n) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:]))]\n",
        "\n",
        "    def fwd_prop(self, X):\n",
        "        X_ell = X   # X_0 (input layer)\n",
        "        for W, b, theta in zip(self.weights, self.biases, self.thetas):\n",
        "            X_ell = theta((X_ell @ W) + b) # compute X_l, 0 < l <= L\n",
        "        return X_ell # X_L (output layer)"
      ],
      "metadata": {
        "id": "i60AbZ7kM-SE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now define the loss function we will use to train our model, along with the accuracy model <>... ðŸš§ðŸš§ðŸš§ðŸš§. Like lab 1, we will use the cross entropy loss since it maximum likelihood and mutually exlusive outcomes and is fit for the classification task at hand."
      ],
      "metadata": {
        "id": "TSNVfnB7Iwdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loss function\n",
        "class CrossEntropy:\n",
        "    @staticmethod\n",
        "    def __call__(p_hat, ys):\n",
        "        return -np.sum(ys*np.log(p_hat))\n",
        "\n",
        "    # derivative with respect to input (prediction distributions)\n",
        "    @staticmethod\n",
        "    def derivative(p_hat, ys):\n",
        "        return -ys/p_hat\n",
        "\n",
        "    # gradient with respect to s^(3), the input to softmax layer\n",
        "    @staticmethod\n",
        "    def grad_softmaxlogits(logits, ys):\n",
        "        return Softmax()(logits) - ys # p_hat - ys"
      ],
      "metadata": {
        "id": "z28ycr7EPLEB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also define the `accuracy` function to compute the mean number of misclassifications, giving us an idea of how likely it is for the network to arrive at an incorrect label."
      ],
      "metadata": {
        "id": "9EDfOQALziwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy of prediction\n",
        "def accuracy(p_hat, ys):\n",
        "    return np.mean(np.argmax(ys, axis=1) == np.argmax(p_hat, axis=1))"
      ],
      "metadata": {
        "id": "4a5RHYFSzh67"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will run a forward pass on an untrained, newly initialized, network and test for its accuracy to make sure our implementation is functional thus far. We source the data from section 1.3 just for this purpose."
      ],
      "metadata": {
        "id": "SaSIXe7x41F3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create neural network\n",
        "F, H, K = 784, 1000, 10\n",
        "nn = Network([F, H, K], [ReLU(), Softmax()])\n",
        "\n",
        "# forward pass, loss, and accuracy on random init\n",
        "p_hat    = nn.fwd_prop(X_train)[-1]\n",
        "accur    = accuracy(p_hat, ys_train)\n",
        "avg_loss = CrossEntropy()(p_hat, ys_train)/X_train.shape[0]\n",
        "print(\"Loss = {}, Accuracy = {}\".format(avg_loss, accur))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NC9XcHR45IcX",
        "outputId": "8f71fa8e-bda4-4e8f-8b13-e7df3d7ecf79"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss = 2.727745239229891, Accuracy = 0.0468\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Backpropagation Derivation\n",
        "\n",
        "We derive the gradient of the loss function with respect to the model parameters, to be able to update them."
      ],
      "metadata": {
        "id": "bRSG-YED5RFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# due to time-availability constraints, we will constrain training to a 3 layer\n",
        "# network with pre-specified structure, i.e. not generic\n",
        "def three_layer_multiclass_CE_back_prop(self, X, ys, loss):\n",
        "    assert len(self.sizes) == 3\n",
        "    assert isinstance(self.thetas[0], ReLU)\n",
        "    assert isinstance(self.thetas[1], Softmax)\n",
        "    assert isinstance(loss, CrossEntropy)\n",
        "    \n",
        "    # get summations and activations\n",
        "    X_ells = [X]\n",
        "    S_ells = []\n",
        "    for W, b, theta in zip(self.weights, self.biases, self.thetas):\n",
        "        S_ell = (X_ells[-1] @ W) + b\n",
        "        np.append(X_ells, [theta(S_ell)], axis=0)\n",
        "        np.append(S_ells, [S_ell], axis=0)\n",
        "    \n",
        "    loss.grad_softmaxlogits()\n",
        "\n",
        "    delta_1 = ReLU().prime(x)\n",
        "    delta_2 = \n",
        "\n",
        "    grad_W1 = \n",
        "    grad_W2 = \n",
        "    grad_b1 = \n",
        "    grad_b2 = \n",
        "\n",
        "    grad_weights = np.array([grad_W1, grad_W2], dtype=object)\n",
        "    grad_weights = np.array([grad_b1, grad_b2], dtype=object)\n",
        "\n",
        "    return grad_weights, grad_biases\n",
        "\n",
        "# run batch backprop over a generic (fully-connected) network with vector-valued\n",
        "# activations too; for inspiration see: https://sgugger.github.io/a-simple-neural-net-in-numpy.html\n",
        "def back_prop(self, X, ys):\n",
        "    raise NotImplementedError # todo: for future me\n",
        "    return grad_weights, grad_biases\n",
        "\n",
        "# expand Network definition\n",
        "Network.back_prop = three_layer_multiclass_CE_back_prop # todo: generic back_prop\n",
        "nn.back_prop = three_layer_multiclass_CE_back_prop      # todo: generic back_prop"
      ],
      "metadata": {
        "id": "ztqBfrR863pf"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Learning\n",
        "\n",
        "Training and data"
      ],
      "metadata": {
        "id": "GeXxzhsBPAaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data loader and associated helper functions\n",
        "def loadDataGDrive():\n",
        "    with np.load('/content/drive/MyDrive/Colab Notebooks/notMNIST.npz') as data:\n",
        "        data, targets = data[\"images\"], data[\"labels\"]\n",
        "        \n",
        "        np.random.seed(521)\n",
        "        rand_idx = np.arange(len(data))\n",
        "        np.random.shuffle(rand_idx)\n",
        "        \n",
        "        data = data[rand_idx] / 255.0\n",
        "        targets = targets[rand_idx].astype(int)\n",
        "        \n",
        "        train_data, train_target = data[:10000], targets[:10000]\n",
        "        valid_data, valid_target = data[10000:16000], targets[10000:16000]\n",
        "        test_data, test_target = data[16000:], targets[16000:]\n",
        "        train_target, valid_target, test_target = convert_onehot(train_target, \n",
        "                                                                 valid_target, \n",
        "                                                                 test_target)\n",
        "    return train_data, valid_data, test_data, train_target, valid_target, test_target\n",
        "\n",
        "def convert_onehot(train_target, valid_target, test_target):\n",
        "    new_train = np.zeros((train_target.shape[0], 10))\n",
        "    new_valid = np.zeros((valid_target.shape[0], 10))\n",
        "    new_test = np.zeros((test_target.shape[0], 10))\n",
        "\n",
        "    for item in range(0, train_target.shape[0]):\n",
        "        new_train[item][train_target[item]] = 1\n",
        "    for item in range(0, valid_target.shape[0]):\n",
        "        new_valid[item][valid_target[item]] = 1\n",
        "    for item in range(0, test_target.shape[0]):\n",
        "        new_test[item][test_target[item]] = 1\n",
        "    return new_train, new_valid, new_test\n",
        "\n",
        "def shuffle(data, target):\n",
        "    np.random.seed(421)\n",
        "    rand_idx = np.random.permutation(len(data))\n",
        "    return data[rand_idx], target[rand_idx]\n",
        "\n",
        "# load data\n",
        "x_train, x_valid, x_test, ys_train, ys_valid, ys_test = loadDataGDrive()"
      ],
      "metadata": {
        "id": "USALvma_PC3q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# flatten datavectors\n",
        "X_train = x_train.reshape(x_train.shape[0], -1)\n",
        "X_valid = x_valid.reshape(x_valid.shape[0], -1)\n",
        "X_test  = x_test.reshape(x_test.shape[0], -1)\n",
        "\n",
        "# data for training and testing our neural network\n",
        "Xs = np.array([X_train, X_valid, X_test], dtype=object)    # training, validation, and testing data\n",
        "Ys = np.array([ys_train, ys_valid, ys_test], dtype=object) # training, validation, and testing labels"
      ],
      "metadata": {
        "id": "4rmh5IYe9BEK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_learning_history(title, loss_hist, acc_hist):\n",
        "    # loss_hist = [train_loss, valid_loss, test_loss]\n",
        "    # acc_hist  = [train_acc, valid_acc, test_acc]\n",
        "    f, ax  = plt.subplots(1, 2, figsize=(8,4))\n",
        "    labels = [\"Train\", \"Validation\", \"Test\"]\n",
        "\n",
        "    for i in range(loss_hist.shape[1]):\n",
        "        ax[0].plot(loss_hist[:, i], label=labels[i]) # plot loss history on left\n",
        "        ax[1].plot(acc_hist[:, i], label=labels[i])  # plot accuracy history on right\n",
        "    \n",
        "    # add lables and title\n",
        "    ax[0].set_xlabel(\"Epochs\")\n",
        "    ax[0].set_ylabel(\"Loss\")\n",
        "    ax[0].legend()\n",
        "    ax[1].set_xlabel(\"Epochs\")\n",
        "    ax[1].set_ylabel(\"Accuracy\")\n",
        "    ax[1].legend()\n",
        "    f.suptitle(title, fontsize=14, y=1.03)\n",
        "    return plt.tight_layout()"
      ],
      "metadata": {
        "id": "5te4wQeM-3g9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now write the training function for our network."
      ],
      "metadata": {
        "id": "FQaLNrY7MJAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_three_layer_multiclass_CE(self, Xs, Ys, loss, epochs, alpha=0.1, gamma=0.9):\n",
        "    # Xs = [X_train, X_valid (optional), X_test (optional)]\n",
        "    # Ys = [ys_train, ys_valid (optional), ys_test (optional)]\n",
        "    loss_hist = np.empty((0,Xs.shape[0]), float)\n",
        "    acc_hist  = np.empty((0,Xs.shape[0]), float)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # shuffle data on each epoch\n",
        "        grad_weights, grad_biases = back_prop(Xs, Ys)\n",
        "\n",
        "        # get loss\n",
        "        p_hat    = nn.fwd_prop(X_train)[-1]\n",
        "        accur    = accuracy(p_hat, ys_train)\n",
        "        avg_loss = CrossEntropy()(p_hat, ys_train)/X_train.shape[0]\n",
        "\n",
        "        # print progress\n",
        "        if epoch % 20 == 0:\n",
        "            print()\n",
        "\n",
        "    return self.weights, self.biases, loss_hist, acc_hist\n",
        "\n",
        "# expand Network defintion\n",
        "Network.train = train\n",
        "nn.train = train"
      ],
      "metadata": {
        "id": "BshhqOy7MMDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test it:"
      ],
      "metadata": {
        "id": "3wK00-78MaKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, _, loss_history, accuracy_hostory = nn.train(Xs, Ys, CrossEntropy(), 200)"
      ],
      "metadata": {
        "id": "ILFIm_o9McKq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}