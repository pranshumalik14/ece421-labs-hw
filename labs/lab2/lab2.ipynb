{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1yAohXBUxBSMcoMJD6jAZ4wnN7GDJ869G",
      "authorship_tag": "ABX9TyMIr4IIQHegbbMfCQEBm+Kp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranshumalik14/ece421-labs-hw/blob/main/labs/lab2/lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 2: Neural Networks\n",
        "\n",
        "In this lab, we will be creating a ."
      ],
      "metadata": {
        "id": "YtLiBTx7KvEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Neural Networks using Numpy\n",
        "\n",
        "We implement.\n"
      ],
      "metadata": {
        "id": "NBl4puliM4oc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Helper Functions and Network Definitions"
      ],
      "metadata": {
        "id": "jGWsXc7S6reh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Gff6Ah1DL3Oy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first start with defining the activation functions along with their derivatives, which we will later be able to use in our neural networks."
      ],
      "metadata": {
        "id": "_P4168HVCneW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# activation functions\n",
        "class ReLU:\n",
        "    # relu = max(x, 0)    \n",
        "    @staticmethod\n",
        "    def __call__(x):\n",
        "        return\n",
        "    # derivative of relu\n",
        "    @staticmethod\n",
        "    def prime(x):\n",
        "        return\n",
        "\n",
        "class Softmax:\n",
        "    # \n",
        "    x = x - x.max(axis=1, keepdims=True)\n",
        "    return exp(x)/exp(x).sum(axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "nFjDwg6nCukr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we define the `Network` class that will be used to define an arbitrary network with dense (fully-connected) layers. The class definition and functionality will expand as we progress through each section. We begin with defining the functions that create the network structure, model parameters, their intializatin, and the forward pass through the network.\n",
        "\n",
        "<TODO: ðŸš§ theory behind NN structure, notation>\n",
        "\n",
        "<TODO: ðŸš§ add info about Xavier initialization>"
      ],
      "metadata": {
        "id": "7CajgVMRH1BU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network:\n",
        "    def __init__(self, layer_sizes, layer_activations):\n",
        "        self.sizes   = layer_sizes\n",
        "        self.thetas  = layer_activations\n",
        "        self.biases  = []\n",
        "        self.weights = []\n",
        "    \n",
        "    def fwd_prop():\n",
        "        pass"
      ],
      "metadata": {
        "id": "i60AbZ7kM-SE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now define the loss function we will use to train our model, along with the accuracy model <>... ðŸš§ðŸš§ðŸš§ðŸš§"
      ],
      "metadata": {
        "id": "TSNVfnB7Iwdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loss function\n",
        "class CrossEntropy:\n",
        "\n",
        "    def average_ce(target, prediction):\n",
        "        # TODO\n",
        "        pass\n",
        "\n",
        "    def grad_ce(target, logits):\n",
        "        # TODO\n",
        "        pass"
      ],
      "metadata": {
        "id": "z28ycr7EPLEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Backpropagation Derivation\n",
        "\n",
        "We derive the gradient of the loss function with respect to the model parameters, to be able to update them."
      ],
      "metadata": {
        "id": "bRSG-YED5RFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ztqBfrR863pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Learning\n",
        "\n",
        "Training and data"
      ],
      "metadata": {
        "id": "GeXxzhsBPAaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data loader and associated helper functions\n",
        "def loadDataGDrive():\n",
        "    with np.load('/content/drive/MyDrive/Colab Notebooks/notMNIST.npz') as data:\n",
        "        data, targets = data[\"images\"], data[\"labels\"]\n",
        "        \n",
        "        np.random.seed(521)\n",
        "        rand_idx = np.arange(len(data))\n",
        "        np.random.shuffle(rand_idx)\n",
        "        \n",
        "        data = data[rand_idx] / 255.0\n",
        "        targets = targets[rand_idx].astype(int)\n",
        "        \n",
        "        train_data, train_target = data[:10000], targets[:10000]\n",
        "        valid_data, valid_target = data[10000:16000], targets[10000:16000]\n",
        "        test_data, test_target = data[16000:], targets[16000:]\n",
        "        train_target, valid_target, test_target = convert_onehot(train_target, \n",
        "                                                                 valid_target, \n",
        "                                                                 test_target)\n",
        "    return train_data, valid_data, test_data, train_target, valid_target, test_target\n",
        "\n",
        "def convert_onehot(train_target, valid_target, test_target):\n",
        "    new_train = np.zeros((train_target.shape[0], 10))\n",
        "    new_valid = np.zeros((valid_target.shape[0], 10))\n",
        "    new_test = np.zeros((test_target.shape[0], 10))\n",
        "\n",
        "    for item in range(0, train_target.shape[0]):\n",
        "        new_train[item][train_target[item]] = 1\n",
        "    for item in range(0, valid_target.shape[0]):\n",
        "        new_valid[item][valid_target[item]] = 1\n",
        "    for item in range(0, test_target.shape[0]):\n",
        "        new_test[item][test_target[item]] = 1\n",
        "    return new_train, new_valid, new_test\n",
        "\n",
        "def shuffle(data, target):\n",
        "    np.random.seed(421)\n",
        "    rand_idx = np.random.permutation(len(data))\n",
        "    return data[rand_idx], target[rand_idx]\n",
        "\n",
        "# load data\n",
        "x_train, x_valid, x_test, ys_train, ys_valid, ys_test = loadDataGDrive()"
      ],
      "metadata": {
        "id": "USALvma_PC3q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# flatten datavectors\n",
        "X_train = x_train.reshape(x_train.shape[0], -1)\n",
        "X_valid = x_valid.reshape(x_valid.shape[0], -1)\n",
        "X_test  = x_test.reshape(x_test.shape[0], -1)\n",
        "\n",
        "# data for training and testing our neural network\n",
        "Xs = np.array([X_train, X_valid, X_test], dtype=object)    # training, validation, and testing data\n",
        "Ys = np.array([ys_train, ys_valid, ys_test], dtype=object) # training, validation, and testing labels"
      ],
      "metadata": {
        "id": "4rmh5IYe9BEK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_learning_history(title, loss_hist, acc_hist):\n",
        "    # loss_hist = [train_loss, valid_loss, test_loss]\n",
        "    # acc_hist  = [train_acc, valid_acc, test_acc]\n",
        "    f, ax  = plt.subplots(1, 2, figsize=(8,4))\n",
        "    labels = [\"Train\", \"Validation\", \"Test\"]\n",
        "\n",
        "    for i in range(loss_hist.shape[1]):\n",
        "        ax[0].plot(loss_hist[:, i], label=labels[i]) # plot loss history on left\n",
        "        ax[1].plot(acc_hist[:, i], label=labels[i])  # plot accuracy history on right\n",
        "    \n",
        "    # add lables and title\n",
        "    ax[0].set_xlabel(\"Epochs\")\n",
        "    ax[0].set_ylabel(\"Loss\")\n",
        "    ax[0].legend()\n",
        "    ax[1].set_xlabel(\"Epochs\")\n",
        "    ax[1].set_ylabel(\"Accuracy\")\n",
        "    ax[1].legend()\n",
        "    f.suptitle(title, fontsize=14, y=1.03)\n",
        "    return plt.tight_layout()"
      ],
      "metadata": {
        "id": "5te4wQeM-3g9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}