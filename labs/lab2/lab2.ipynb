{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1yAohXBUxBSMcoMJD6jAZ4wnN7GDJ869G",
      "authorship_tag": "ABX9TyOy8sk9jKHpH8cjfDf3vcQn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranshumalik14/ece421-labs-hw/blob/main/labs/lab2/lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 2: Neural Networks\n",
        "\n",
        "In this lab, we will be creating a ."
      ],
      "metadata": {
        "id": "YtLiBTx7KvEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Neural Networks using Numpy\n",
        "\n",
        "We implement.\n"
      ],
      "metadata": {
        "id": "NBl4puliM4oc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Helper Functions and Network Definitions"
      ],
      "metadata": {
        "id": "jGWsXc7S6reh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Gff6Ah1DL3Oy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    x = x - x.max(axis=1, keepdims=True)\n",
        "    return exp(x)/exp(x).sum(axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "i60AbZ7kM-SE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation of a neural network using only Numpy - trained using gradient descent with momentum\n",
        "def relu(x):\n",
        "    # TODO\n",
        "    pass\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    # TODO\n",
        "    pass\n",
        "\n",
        "def compute_layer(x, w, b):\n",
        "    # TODO\n",
        "    pass\n",
        "\n",
        "def average_ce(target, prediction):\n",
        "    # TODO\n",
        "    pass\n",
        "\n",
        "def grad_ce(target, logits):\n",
        "    # TODO\n",
        "    pass"
      ],
      "metadata": {
        "id": "z28ycr7EPLEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Backpropagation Derivation\n",
        "\n",
        "We derive the gradient of the loss function with respect to the model parameters, to be able to update them."
      ],
      "metadata": {
        "id": "bRSG-YED5RFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ztqBfrR863pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Learning\n",
        "\n",
        "Training and data"
      ],
      "metadata": {
        "id": "GeXxzhsBPAaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data loader and associated helper functions\n",
        "def loadDataGDrive():\n",
        "    with np.load('/content/drive/MyDrive/Colab Notebooks/notMNIST.npz') as data:\n",
        "        data, targets = data[\"images\"], data[\"labels\"]\n",
        "        \n",
        "        np.random.seed(521)\n",
        "        rand_idx = np.arange(len(data))\n",
        "        np.random.shuffle(rand_idx)\n",
        "        \n",
        "        data = data[rand_idx] / 255.0\n",
        "        targets = targets[rand_idx].astype(int)\n",
        "        \n",
        "        train_data, train_target = data[:10000], targets[:10000]\n",
        "        valid_data, valid_target = data[10000:16000], targets[10000:16000]\n",
        "        test_data, test_target = data[16000:], targets[16000:]\n",
        "        train_target, valid_target, test_target = convert_onehot(train_target, \n",
        "                                                                 valid_target, \n",
        "                                                                 test_target)\n",
        "    return train_data, valid_data, test_data, train_target, valid_target, test_target\n",
        "\n",
        "def convert_onehot(train_target, valid_target, test_target):\n",
        "    new_train = np.zeros((train_target.shape[0], 10))\n",
        "    new_valid = np.zeros((valid_target.shape[0], 10))\n",
        "    new_test = np.zeros((test_target.shape[0], 10))\n",
        "\n",
        "    for item in range(0, train_target.shape[0]):\n",
        "        new_train[item][train_target[item]] = 1\n",
        "    for item in range(0, valid_target.shape[0]):\n",
        "        new_valid[item][valid_target[item]] = 1\n",
        "    for item in range(0, test_target.shape[0]):\n",
        "        new_test[item][test_target[item]] = 1\n",
        "    return new_train, new_valid, new_test\n",
        "\n",
        "def shuffle(data, target):\n",
        "    np.random.seed(421)\n",
        "    rand_idx = np.random.permutation(len(data))\n",
        "    return data[rand_idx], target[rand_idx]\n",
        "\n",
        "# load data\n",
        "x_train, x_valid, x_test, ys_train, ys_valid, ys_test = loadDataGDrive()"
      ],
      "metadata": {
        "id": "USALvma_PC3q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# flatten datavectors\n",
        "X_train = x_train.reshape(x_train.shape[0], -1)\n",
        "X_valid = x_valid.reshape(x_valid.shape[0], -1)\n",
        "X_test  = x_test.reshape(x_test.shape[0], -1)\n",
        "\n",
        "# data for training and testing our neural network\n",
        "Xs = np.array([X_train, X_valid, X_test], dtype=object)    # training, validation, and testing data\n",
        "Ys = np.array([ys_train, ys_valid, ys_test], dtype=object) # training, validation, and testing labels"
      ],
      "metadata": {
        "id": "4rmh5IYe9BEK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_learning_history(title, loss_hist, acc_hist):\n",
        "    # loss_hist = [train_loss, valid_loss, test_loss]\n",
        "    # acc_hist  = [train_acc, valid_acc, test_acc]\n",
        "    f, ax  = plt.subplots(1, 2, figsize=(8,4))\n",
        "    labels = [\"Train\", \"Validation\", \"Test\"]\n",
        "\n",
        "    for i in range(loss_hist.shape[1]):\n",
        "        ax[0].plot(loss_hist[:, i], label=labels[i]) # plot loss history on left\n",
        "        ax[1].plot(acc_hist[:, i], label=labels[i])  # plot accuracy history on right\n",
        "    \n",
        "    # add lables and title\n",
        "    ax[0].set_xlabel(\"Epochs\")\n",
        "    ax[0].set_ylabel(\"Loss\")\n",
        "    ax[0].legend()\n",
        "    ax[1].set_xlabel(\"Epochs\")\n",
        "    ax[1].set_ylabel(\"Accuracy\")\n",
        "    ax[1].legend()\n",
        "    f.suptitle(title, fontsize=14, y=1.03)\n",
        "    return plt.tight_layout()"
      ],
      "metadata": {
        "id": "5te4wQeM-3g9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}