\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage[top=1.2in, textwidth=6.5in]{geometry}

% set up margins
\textheight=9in
\parskip=.07in
\parindent=0in
\headheight=23pt
\pagestyle{fancy}

% set up header
\newcommand{\setheader}[5]
{
	\lhead{{\sc #1}\\{\sc #2} ({\small \it #3})}
	\rhead{{\bf #4}\\{#5}}
}

% set up some shortcuts
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\renewcommand{\vec}[1]{\mathbf{#1}}

\begin{document}
    \setheader{ECE421: Introduction to Machine Learning}
    {Homework Assignment 3}{\today}{Pranshu Malik}{1004138916}

    \textbf{Solution 1: Taylor Approximation}

    Given a scalar function $f: \R^n \to \R$, its first-order Taylor 
    approximation about a point $\vec{z}\in\R^n$ is given by $\widehat{f} 
    (\vec{x}) = f(\vec{z}) + \nabla f(\vec{z})^\top (\vec{x}-\vec{z})$. We 
    will consider the function, 
    \[
        E(u,v) = e^u + e^{2v} + e^{uv} + u^2 - 3uv + 4v^2 - 3u -5v,
    \]
    where $u$ and $v$ are scalars.

    a) The first-order Taylor approximation of $E(u + \Delta u, v+\Delta v)$ 
    about $\vec{z} = \mat{u&v}^\top = \mat{0&0}^\top$ is 
    \[
        \widehat{E}(\Delta u, \Delta v) = E(0,0) + \nabla E(0,0)^\top 
        \mat{\Delta u \\ \Delta v}.
    \]
    We have $E(0,0) = 3$ and $\nabla E(u,v) = \mat{\frac{\partial E}{\partial 
    u}\vspace{0.1cm} \\ \frac{\partial E}{\partial v}} = \mat{e^u ve^{uv} + 
    2u -3v -3\\ 2e^{2v} + ue^{uv} + 2u + 8v -5}$. Substituting in 
    above, we get
    \[
        \widehat{E}(\Delta u,\Delta v)=3+\mat{-2&-3}\mat{\Delta u\\ \Delta v}
        = 3 - 2\Delta u - 3\Delta v.
    \]
    Comparing the expression above, we can infer $a = 3, a_u = -2, a_v = -3$.

    b) To minimize $\widehat{E}$ over all possible steps of $\Delta u$ and
    $\Delta v$, such that the stepsize $||(\Delta u, \Delta v)|| = 0.5$, we 
    use our knowledge of gradient descent. Particularly, the step should be 
    in the direction of $-\nabla\widehat{E}$. Therefore,
    \[
        \mat{\Delta u\\\Delta v} = -c\nabla\widehat{E}(0,0) = c\mat{2\\3},
    \]
    for some $c>0$. Now, applying the stepsize constraint, we have $0.5 = 
    c\sqrt{2^2 + 3^2} \iff c = \frac{1}{2\sqrt{13}}$. Hence, the minimizer 
    at $(0,0)$ is
    \[
        \mat{\Delta u^\star\\\Delta v^\star} = \frac{1}{2\sqrt{13}}\mat{2\\3}.
    \]

    \vspace{0.5cm}

    \textbf{Solution 2: Logistic and Multi-class Softmax Regression (Midterm 
    2019, Problem 4)}

    We consider a multi-class softmax regression model to classify datavectors
    $\vec{x}_i\in\R^{d+1}$ into class labels $y_i\in\{1,2\}$. The 
    weightvectors $\vec{w}(1), \vec{w}(2)$ correspond to classes $1$ and $2$ 
    respectively. Given any datavector $\vec{x}_n\in\R^{d+1}$, the model 
    yields the probability estimate for true class $y_n \in \{1, 2\}$ as
    \begin{equation}
        \label{eqn:softmax}
        \hat{p}^{\text{SM}}_{\Omega}(y_n\mid\vec{x}_n) = \frac{e^{\vec{w}(y_n)
        ^\top\vec{x}_n}}{\sum_{i=1}^2e^{\vec{w}(i)^\top\vec{x}_n}},
    \end{equation}
    where $\Omega = \{\vec{w}(1), \vec{w}(2)\}$. The corresponding loss 
    function for the training sample $(\vec{x}_n,y_n)$ is
    \begin{equation}
        \label{eqn:loss}
        e_n^{\text{SM}}(\Omega) = -\log\left(\hat{p}^{\text{SM}}_{\Omega}
        (y_n\mid\vec{x}_n)\right).
    \end{equation}
    Similarly, we consider the binary logistic regression model from Homework 
    2, that produces the probability estimate for the datavector belonging to
    class $1$ as
    \begin{equation}
        \label{eqn:logistic}
        \hat{p}^{\text{LR}}_{\vec{w}}(y_n=1\mid\vec{x}_n) = \frac{e^{\vec{w}
        ^\top\vec{x}_n}}{1+e^{\vec{w}^\top\vec{x}_n}}.
    \end{equation}
    Subsequently, the probability estimate for class $2$ is $\hat{p}^{\text
    {LR}}_{\vec{w}}(y_n=2\mid\vec{x}_n) = 1-\hat{p}^{\text{LR}}_{\vec{w}}
    (y_n=1\mid\vec{x}_n)$. The single- sample loss function remains identical
    to softmax and is given by
    \begin{equation}
        e_n^{\text{LR}}(\vec{w}) = -\log\left(\hat{p}^{\text{LR}}_{\vec{w}}
        (y_n\mid\vec{x}_n)\right).
    \end{equation}

    a) The gradient of $e_n^{\text{SM}}$ will have to be calculated for both
    cases of $y_n$. We first expand the single-sample loss function in 
    \eqref{eqn:loss} as
    \begin{equation}
        \label{eqn:loss_rewritten}
        e_n^{\text{SM}}(\Omega) = -\vec{w}(y_n)^\top\vec{x}_n+\log\left
        (\sum_{i=1}^2e^{\vec{w}(i)^\top\vec{x}_n}\right).
    \end{equation}
    Now, evaluating the gradient about $\vec{w}(j)$ some $j\in\{1,2\}$ we have
    \begin{equation}
        \label{eqn:loss_grad}
        \nabla_{\vec{w}(j)}e^{\text{SM}}_n(\Omega) = 
        \underbrace{\nabla_{\vec{w}(j)}\big[-\vec{w}(y_n)^\top\vec{x}_n\big]}_
        {\nabla_{\vec{w}(j)}e_{n,1}^{\text{SM}}}
        + \underbrace{\nabla_{\vec{w}(j)}\Bigg[\log\left
        (\sum_{i=1}^2e^{\vec{w}(i)^\top\vec{x}_n}\right)\Bigg]}_
        {\nabla_{\vec{w}(j)}e_{n,2}^{\text{SM}}}.
    \end{equation}
    Further evaluating the two subparts in \eqref{eqn:loss_grad}, we get
    \begin{align}
        \label{eqn:loss_grad_pt_1}
        \nabla_{\vec{w}(j)}e_{n,1}^{\text{SM}} &= 
        \begin{cases}
            0 & y_n \neq j\\
            -\vec{x}_n & y_n = j
        \end{cases},\\
        \label{eqn:loss_grad_pt_2}
        \nabla_{\vec{w}(j)}e_{n,2}^{\text{SM}} &= \frac{e^{\vec{w}(j)^\top\vec
        {x}_n}\nabla_{\vec{w}(j)}(\vec{w}(j)^\top\vec{x}_n)}{\sum_{i=1}^2e^
        {\vec{w}(i)^\top\vec{x}_n}} = \vec{x}_n\frac{e^{\vec{w}(j)^\top\vec{x}
        _n}}{\sum_{i=1}^2e^{\vec{w}(i)^\top\vec{x}_n}}.
    \end{align}
    Finally, after combining \eqref{eqn:loss_grad_pt_1} and 
    \eqref{eqn:loss_grad_pt_2}, we get the gradient of the loss 
    function over $y_n,j\in\{1,2\}$ as
    \begin{equation}
        \label{eqn:sm_loss_grad_combined}
        \nabla_{\vec{w}(j)}e^{\text{SM}}_n(\Omega) = 
        \begin{cases}
            \vec{x}_n\frac{e^{\vec{w}(j)^\top\vec{x}_n}}{\sum_{i=1}^2e^{\vec
            {w}(i)^\top\vec{x}_n}} & y_n \neq j\\ 
            \vec{x}_n\left(\frac{e^{\vec{w}(j)^\top\vec{x}_n}}{\sum_{i=1}^2e^
            {\vec{w}(i)^\top\vec{x}_n}}-1\right) & y_n = j
        \end{cases}.
    \end{equation}

    b) For the classification probability estimates to be identical across
    logistic and softmax regression, we need to have the following equality,
    \begin{align}
        \hat{p}^{\text{SM}}_{\Omega}(y_n=1\mid\vec{x}_n)=\hat{p}^{\text{LR}}
        _{\vec{w}}(y_n=1\mid\vec{x}_n) &\iff \frac{e^{\vec{w}(1)^\top\vec{x}
        _n}}{e^{\vec{w}(1)^\top\vec{x}_n}+e^{\vec{w}(2)^\top\vec{x}_n}} = 
        \frac{e^{\left(\vec{w}(1)-\vec{w}(2)\right)^\top\vec{x}_n}}{1+e^{\left
        (\vec{w}(1)-\vec{w}(2)\right)^\top\vec{x}_n}} = 
        \frac{e^{\vec{w}^\top\vec{x}_n}}{1+e^{\vec{w}^\top\vec{x}_n}}\\
        \label{eqn:prob_eq}
        &\iff \vec{w} = \vec{w}(1) - \vec{w}(2).
    \end{align}
    Thus, both models are identical if and only if \eqref{eqn:prob_eq} holds.

    c) The stochastic gradient descent (SGD) involves the following weight 
    update rule for a randomly selected datavector index $n$ in the dataset
    \begin{equation}
        \vec{w}_{k+1} = \vec{w}_k - \epsilon\nabla_{\vec{w}_k}e_n,
    \end{equation}
    where $k\in\N$ is the iteration or epoch number and $\epsilon>0$ is the 
    learning rate. Having already calculated the gradient of single-sample 
    loss in softmax regression, we invoke the same for logistic regression 
    below from our work in Homework 2.
    \begin{equation}
        \nabla_{\vec{w}}e_n^{\text{LR}}(\vec{w}) = 
        \begin{cases}
            \vec{x}_n\frac{-e^{-\vec{w}^\top\vec{x}_n}}{1+e^{-\vec{w}^\top\vec
            {x}_n}} & y_n = 1\\
            \vec{x}_n\frac{e^{\vec{w}^\top\vec{x}_n}}{1+e^{\vec{w}^\top\vec
            {x}_n}} & y_n = 2
        \end{cases}.
    \end{equation}
    Therefore, the SGD weight updates for logistic regression for $y_n=1$ 
    will be
    \begin{equation}
        \label{eqn:lr_sgd}
        \vec{w}_{k+1} = \vec{w}_k + \epsilon^{\text{LR}}
        \frac{\vec{x}_n e^{-\vec{w}_k^\top\vec{x}_n}}{1+e^{-\vec{w}_k^\top\vec
        {x}_n}}.
    \end{equation}
    Similarly, using \eqref{eqn:sm_loss_grad_combined}, the the SGD weight 
    updates for softmax regression for $y_n=1$ will be
    \begin{align}
        \label{eqn:sm_sgd_w_1}
        \vec{w}_{k+1}(1) &= \vec{w}_k(1) + \epsilon^{\text{SM}}\frac{\vec{x}_n
        e^{\vec{w}_k(2)^\top\vec{x}_n}}{e^{\vec{w}_k(1)^\top\vec{x}_n}+e^{\vec
        {w}_k(2)^\top\vec{x}_n}}\\
        \label{eqn:sm_sgd_w_2}
        \vec{w}_{k+1}(2) &= \vec{w}_k(2) - \epsilon^{\text{SM}}\frac{\vec{x}_n
        e^{\vec{w}_k(2)^\top\vec{x}_n}}{e^{\vec{w}_k(1)^\top\vec{x}_n}+e^{\vec
        {w}_k(2)^\top\vec{x}_n}}.
    \end{align} 
    Now, comparing \eqref{eqn:lr_sgd} to $\eqref{eqn:sm_sgd_w_1}
    -\eqref{eqn:sm_sgd_w_2}$ and utilizing \eqref{eqn:prob_eq} to simplify 
    the gradient terms in softmax regression, we can infer that
    \begin{equation}
        \vec{w}_k + \epsilon^{\text{LR}}\frac{\vec{x}_n}{1+e^{\vec{w}
        _k^\top\vec{x}_n}}=\underbrace{\left(\vec{w}_k(1)-\vec{w}_k(2)\right)}
        _{\vec{w}_k} + 2\epsilon^{\text{SM}}\frac{\vec{x}_n}{1+e^{\vec{w}_k
        ^\top\vec{x}_n}}
        \iff \epsilon^{\text{LR}} = 2\epsilon^{\text{SM}},
    \end{equation}
    can only make the evolution of SGD weight updates identical across 
    logistic and softmax regression.
\end{document}