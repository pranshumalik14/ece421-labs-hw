\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage[top=1.2in, textwidth=6.5in]{geometry}

% set up margins
\textheight=9in
\parskip=.07in
\parindent=0in
\headheight=23pt
\pagestyle{fancy}

% set up header
\newcommand{\setheader}[5]
{
	\lhead{{\sc #1}\\{\sc #2} ({\small \it #3})}
	\rhead{{\bf #4}\\{#5}}
}

% set up some shortcuts
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\renewcommand{\vec}[1]{\mathbf{#1}}

\begin{document}
    \setheader{ECE421: Introduction to Machine Learning}
    {Homework Assignment 2}{\today}{Pranshu Malik}{1004138916}

    \textbf{Solution 1: Gradient Computation}

    We use the knowledge of the following rules to calculate the gradients: 
    $\nabla_\mathbf{w}(\mathbf{w}^\top\mathbf{v}) = \nabla_\mathbf{w}(
    \mathbf{v}^\top\mathbf{w}) = \mathbf{v}$ and $\nabla_\mathbf{w}(\mathbf{w}
    ^\top\mathbf{A}\mathbf{w}) = (\mathbf{A}+\mathbf{A}^\top)\mathbf{w}$ for 
    some matrix $\mathbf{A}\in\R^{d\times d}$ and $\mathbf{w}, \mathbf{v}\in
    \R^d$.

    i) The gradient reduces to $\nabla_{\mathbf{w}} f = 2(\mathbf{A}+\mathbf
    {A}^\top)\mathbf{v}$


    ii) $\nabla_{\mathbf{w}} f = (\mathbf{A}+\mathbf{A}^\top)\mathbf{w}$


    iii) $\nabla_{\mathbf{w}} f = \mat{\theta(w_1) & \ldots & \theta(w_d)}
    ^\top$, where $\theta(s) = \frac{e^s}{1+e^s}= \frac{1}{1+e^{-s}}$ is the 
    logistic function.


    iv) $\nabla_{\mathbf{w}} f = \frac{\mathbf{w}}{\sqrt{1+||\mathbf{w}||^2}}$

    \vspace{0.5cm}

    \textbf{Solution 2: Logistic Regression}

    For the dataset 
    $\mathcal{D} = \{(\vec{x}_i, y_i)\}_{i=1}^N$, where $\vec{x}
    _i\in\R^{d+1}$ is an augmented datavector $\mat{1 & x_{i,1} & 
    \ldots & x_{i, d}}^\top$ with label $y_i \in \{-1,1\}$, 
    and given any datavector $\mathbf{x}_n \in \R^{d+1}$, the 
    logistic regression model outputs the probability estimate for 
    the true class $y_n$ as
    \begin{equation}
        \label{eqn:p_hat}
        \hat{p}_\mathbf{w}(y_n\mid \mathbf{x}_n) = \theta\left
        (y_n\mathbf{w}^\top\mathbf{x}_n\right).
    \end{equation}
    The loss function (in-sample error) for the model is defined 
    to be

    \begin{equation}
        \label{eqn:p_hat_loss}
        E_{\text{in}}(\mathbf{w}) = \frac{1}{N}\sum_{n=1}^N 
        \underbrace{-\log\left(\hat{p}_\mathbf{w}(y_n\mid\mathbf{x}
        _n)\right)}_{e_n(\mathbf{w})}
    \end{equation}

    \begin{proof}[\unskip\nopunct]
        a) From \eqref{eqn:p_hat_loss} we can see that since there 
        are only two classes, we can rewite the loss function as
        \begin{align}
            E_{\text{in}}(\mathbf{w}) &=
            \frac{1}{N}\sum_{n=1}^N \big[-I(y_n=1)\log(\hat{p}
            _\mathbf{w}(1\mid \mathbf{x}_n)) -I(y_n=-1)\log(\hat{p}
            _\mathbf{w}(-1\mid \mathbf{x}_n))\big]\nonumber\\
            \label{eqn:p_hat_loss_for_gradient_matrix_form}
            &=\frac{1}{N}\sum_{n=1}^N \Big[-\frac{1+y_n}{2}\log
            (\hat{p}_\mathbf{w}(1\mid \mathbf{x}_n)) -\frac{1-y_n}
            {2}\log(1-\hat{p}_\mathbf{w}(1\mid \mathbf{x}_n))\Big],
        \end{align}
        where $I(y_n=1)$ and $I(y_n=-1)$ are the indicator 
        functions.
    \end{proof}

    \begin{proof}[\unskip\nopunct]
        b) From \eqref{eqn:p_hat} and \eqref{eqn:p_hat_loss} we 
        can see that 
        \[
            e_n(\mathbf{w}) = -\log\left(\hat{p}_\mathbf{w}(y_n 
            \mid \mathbf{x}_n)\right) = \log(1+e^{-y_n\mathbf{w}^
            \top\mathbf{x}_n})    
        \]
        Therefore, we can also rewrite the loss function as
        \begin{equation}
            \label{eqn:p_hat_loss_for_gradient}
            E_{\text{in}}(\mathbf{w}) = \frac{1}{N}\sum_{n=1}^N
            \log(1+e^{-y_n\mathbf{w}^\top\mathbf{x}_n})
        \end{equation}
    \end{proof}

    \begin{proof}[\unskip\nopunct]
        c) Now, from \eqref{eqn:p_hat_loss_for_gradient} we can see
        that the gradient of the loss function is given by
        \begin{align}
            \nabla_{\mathbf{w}}E_{\text{in}}(\mathbf{w}) &= 
            \frac{1}{N}\sum_{n=1}^N\nabla_\mathbf{w}\log(1+e^
            {-y_n\mathbf{w}^\top\mathbf{x}_n}) = 
            \frac{1}{N}\sum_{n=1}^N\frac{-y_n\mathbf{x}_n
            e^{-y_n\mathbf{w}^\top\mathbf{x}_n}}{1+e^{-y_n 
            \mathbf{w}^\top\mathbf{x}_n}}\nonumber\\
            &=\frac{1}{N}\sum_{n=1}^N -y_n\mathbf{x}_n\theta(-y_n
            \mathbf{w}^\top\mathbf{x}_n).
        \end{align}
        Continuing with what we established in Homework 1, here 
        notice that for a confidently missclassified point  
        $y_n\mathbf{w}^\top\mathbf{x}_n\ll0$ which gives 
        $\theta(-y_n\mathbf{w}^\top\mathbf{x}_n) \approx 1$,
        whereas for a confidently correctly classified point, we 
        have $y_n\mathbf{w}^\top\mathbf{x}_n\gg 0$ which gives 
        $\theta(-y_n\mathbf{w}^\top\mathbf{x}_n) \approx 0$, 
        meaning that there is little contribution compared to the
        misclassified case.
    \end{proof}

    \begin{proof}[\unskip\nopunct]
        d) From \eqref{eqn:p_hat_loss_for_gradient_matrix_form} 
        notice that
        \begin{align}
            \nabla_{\mathbf{w}}E_{\text{in}}(\mathbf{w}) &= 
            \frac{1}{N}\sum_{n=1}^N \Big[-\frac{1+y_n}{2}
            \nabla_{\mathbf{w}}\log(\hat{p}_\mathbf{w}(1\mid 
            \mathbf{x}_n)) -\frac{1-y_n}{2}\nabla_{\mathbf{w}}\log
            (1-\hat{p}_\mathbf{w}(1\mid \mathbf{x}_n))\Big]
            \nonumber\\
            &=\frac{1}{2N}\sum_{n=1}^N\big[-(1+y_n)
            (1+e^{-\mathbf{w}
            ^\top\mathbf{x}_n})\nabla_{\mathbf{w}}(1+e^{-\mathbf{w}
            ^\top\mathbf{x}_n})^{-1}-(1-y_n)(1+e^{\mathbf{w}
            ^\top\mathbf{x}_n})\nabla_{\mathbf{w}}(1+e^{\mathbf{w}
            ^\top\mathbf{x}_n})^{-1}\big]\nonumber\\
            &=\frac{1}{2N}\sum_{n=1}^N\big[-(1+y_n)\mathbf{x}
            _n\underbrace{\frac{e^{-\mathbf{w}^\top\mathbf{x}_n}}{
            (1+e^{-\mathbf{w}^\top\mathbf{x}_n})}}_{\theta(-\mathbf
            {w}^\top\mathbf{x}_n) = 1-\theta(\mathbf{w}^\top\mathbf
            {x}_n)}+(1-y_n)\mathbf{x}_n\underbrace{\frac{e^{\mathbf
            {w}^\top\mathbf{x}_n}}{(1+e^{\mathbf{w}^\top\mathbf{x}
            _n})}}_{\theta(\mathbf{w}^\top\mathbf{x}_n)}\big]
            \nonumber\\
            &=\frac{1}{2N}\sum_{n=1}^N \mathbf{x}_n \left(
            2\theta(\mathbf{w}^\top\mathbf{x}_n)-y_n-1\right)
            \nonumber\\
            &=\frac{1}{N}\mathbf{X}^\top\underbrace{\left(
            \boldsymbol{\theta}
            (\mathbf{X}\mathbf{w})-\frac{\mathbf{y}}{2}-
            \frac{\mathbf{I}}{2}\right)}_{p},
        \end{align}
        where $\mathbf{X} = \mat{\mathbf{x}_1^\top&\ldots&\mathbf
        {x}_N^\top}^\top\in \mathbb{R}^{N\times(d+1)}$. Notice 
        that the expression $p$ is similar to $\mathbf{X}\mathbf{w}
        -\mathbf{y}$ in the gradient of the mean squared error 
        loss function (for linear regression) derived in 
        Homework 1.
    \end{proof}

    \vspace{0.5cm}

    \textbf{Solution 3: Midterm 2017, Problem 4}

    We are given $\mathcal{D}=\Big\{\left(\mat{1&1}^\top,1\right),
    \left(\mat{1&0}^\top,-1\right)\Big\} = \{(\mathbf{x}_1, y_1), 
    (\mathbf{x}_2, y_2)\}$ and the corresponding regularized loss 
    function for logistic regression

    \begin{equation}
        E_{\text{in}}(\mathbf{w}) = \lambda ||\mathbf{w}||^2 + 
        \frac{1}{N}\sum_{n=1}^N e_n(\mathbf{w}),
    \end{equation}

    which is a shorthand for \eqref{eqn:p_hat_loss_for_gradient} 
    except for the weight regularization term.

    a) Since $\lambda = 0$, we can evaluate the loss function
    for $\mathbf{w} = \mat{w_1 & w_2}^\top$ as follows
    \[
        E_{\text{in}}(\mathbf{w}) = \log(1+e^{-(w_1 + w_2)}) +
        \log(1+e^{w_1}).
    \]
    This expression can be minimized if $w_1 \to -\infty$ and
    $w_1 + w_2 \to \infty$. This can be realized with $w_2 = -2w_1$
    and $w_1 \to \infty$, which will yield $E_{\text{in}} = 0$.

    b) For $\lambda \gg 0$, we can consider $||w||\ll1$ while 
    minimizing the loss function. For this case, we are given the
    Taylor approximation of the loss function for $e_n(\mathbf{w}) 
    \approx \log(2) - \frac{1}{2}y_n\mathbf{w}^\top\mathbf{x}_n$, 
    which we can substituted into the expression loss in part a) to
    then minimize it, i.e.,
    \begin{align*}
        E_{\text{in}}(\mathbf{w}) &= \lambda (w_1^2+w_2^2) + 
        \log(1+e^{-(w_1 + w_2)}) + \log(1+e^{w_1})\\
        &= \lambda (w_1^2+w_2^2) + \log(2) -\frac{1}{2}(w_1 + w_2)
        + \log(2) + \frac{1}{2}w_1.
    \end{align*}
    Now to minimize the loss, we set
    \begin{align*}
        \frac{\partial E_{\text{in}} }{\partial w_1} &= 0 \implies
         2\lambda w_1 -\frac{1}{2}+\frac{1}{2}=0 \iff w_1= 0\\
         \frac{\partial E_{\text{in}} }{\partial w_2} &= 0 \implies
         2\lambda w_2 -\frac{1}{2} = 0 \iff w_2=\frac{1}{4\lambda}.
    \end{align*}
    Thus, $\mathbf{w}^\star = \mat{0 & \frac{1}{4\lambda}}^\top$ 
    is the minimizer.

\end{document}