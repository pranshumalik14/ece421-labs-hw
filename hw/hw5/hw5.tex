\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage[top=1.2in, textwidth=6.5in]{geometry}

% set up margins
\textheight=9in
\parskip=.07in
\parindent=0in
\headheight=23pt
\pagestyle{fancy}

% set up header
\newcommand{\setheader}[5]
{
	\lhead{{\sc #1}\\{\sc #2} ({\small \it #3})}
	\rhead{{\bf #4}\\{#5}}
}

% set up some shortcuts
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\renewcommand{\vec}[1]{\mathbf{#1}}

\begin{document}
    \setheader{ECE421: Introduction to Machine Learning}
    {Homework Assignment 5}{\today}{Pranshu Malik}{1004138916}

    \textbf{Solution 1: Forward and Backward Propagation}

    Given the network architecture and input $(x^{(0)}_1=2, y=1)$ the 
    squared loss function $e$, and the weight matrices already provided in 
    the example, we will first perform the forward propagation. 
    \begin{itemize}
        \item $\vec{s}^{(1)} = \mat{0.7 & 1}^\top$
        \item $\vec{x}^{(1)} = \mat{0.6044 & 0.7616}^\top$ (in this solution, 
        we exclude bias while expressing all layer activations)
        \item $s^{(2)}_1 = -1.4804$
        \item $x^{(2)}_1 = -0.9015$
        \item $s^{(3)}_1 = -0.8031$
        \item $x^{(3)}_1 = -0.6658$
    \end{itemize}

    Using the information above, we can now proceed to computing the backward 
    propagation.
    \begin{itemize}
        \item $\boldsymbol{\delta}^{(3)} = 2\left(x^{(3)}_1-y\right)
        \tanh^\prime(s^{(3)}_1) = \mat{-1.855}^\top$
        \item $\boldsymbol{\delta}^{(2)} = \mathbf{W}^{(3)}\boldsymbol{\delta}
        ^{(3)}\odot\tanh^\prime(\vec{s}^{(2)}) = \mat{-0.69}^\top$
        \item $\boldsymbol{\delta}^{(1)} = \mathbf{W}^{(2)}\boldsymbol{\delta}
        ^{(2)}\odot\tanh^\prime(\vec{s}^{(1)}) = \mat{-0.44&0.88}^\top$
    \end{itemize}

    Thus, using the information above, we can compute the following.
    \begin{itemize}
        \item $\frac{\partial e}{\partial \mathbf{W}^{(3)}} = \mat{1&\vec{x}^{
        (2)\top}}^\top\boldsymbol{\delta}^{(3)\top} = \mat{-1.85&1.67}^\top$
        \item $\frac{\partial e}{\partial \mathbf{W}^{(2)}} = \mat{1&\vec{x}^{
        (1)\top}}^\top\boldsymbol{\delta}^{(2)\top} = \mat{-0.69&-0.42&-0.53}
        ^\top$
    \end{itemize}

    \vspace{0.5cm}

    \textbf{Solution 2: Neural Networks and Gradients}

    Given the network architecture, we can derive the gradients of the sample 
    loss with respect to the layer activations and weights. The input to the 
    model is the feature or data vector $\vec{x}\in\R^3$ and the
    corresponding ground-truth response $y\in\R$. The model parameters are 
    two scalar weights $\Omega = (w, v)$. We use the squared loss function, 
    $e(\Omega) = (\hat{y}-y)^2$, where $\hat{y}$ is the model output.

    a) We can express $\frac{\partial e}{\partial v} = \frac{\partial e}
    {\partial \hat{y}}\frac{\partial \hat{y}}{\partial v}$. We know that
    $\frac{\partial e}{\partial \hat{y}} = 2(\hat{y}-y) = 2\Delta$, and
    $\frac{\partial \hat{y}}{\partial v} = \frac{\partial x^{(3)}_1}{\partial 
    v}+\frac{\partial vx^{(2)}_2}{\partial v}$. Thus, evaluting, we get $\frac
    {\partial e}{\partial v} = 2\Delta\left(x^{(2)}_2 + vx^{(1)}_1\right)$.

    b) We derive the expressions for the gradients of the loss with respect to
    the various layer activations below. For the same, we first express the 
    model output in terms of first and second layer activations: $\hat{y} = 
    x^{(1)}_3 + v\left(x^{(1)}_2 + vx^{(1)}_1\right)$. Then, we can write
    \begin{itemize}
        \item $\frac{\partial e}{\partial x^{(2)}_2} = \frac{\partial e}
        {\partial \hat{y}}\frac{\partial \hat{y}}{\partial x^{(2)}_2} = 
        2\Delta v$
        \item $\frac{\partial e}{\partial x^{(1)}_1} = \frac{\partial e}
        {\partial \hat{y}}\frac{\partial \hat{y}}{\partial x^{(1)}_1} = 
        2\Delta v^2$
        \item $\frac{\partial e}{\partial x^{(1)}_2} = \frac{\partial e}
        {\partial \hat{y}}\frac{{\partial \hat{y}}}{\partial x^{(1)}_2} = 
        2\Delta v$
        \item $\frac{\partial e}{\partial x^{(3)}_1} = \frac{\partial e}
        {\partial \hat{y}}\frac{\partial \hat{y}}{\partial x^{(3)}_1} = 
        2\Delta$
    \end{itemize}

    c) We know that $\frac{\partial e}{\partial w} = \frac{\partial e}
    {\partial x^{(1)}_3}\frac{\partial x^{(1)}_3}{\partial w} + \frac
    {\partial e}{\partial x^{(1)}_2}\frac{\partial x^{(1)}_2}{\partial w} + 
    \frac{\partial e}{\partial x^{(1)}_1}\frac{\partial x^{(1)}_1}{\partial w}
    $. We can write $\frac{\partial x_i^{(1)}}{\partial w} = \theta^\prime(wx^
    {(0)}_i)x^{(0)}_i$ for $i = 1,2,3$, and by using the above parts, we 
    finally get that $\frac{\partial e}{\partial w} = 2\Delta\sum_{i=1}^3v^
    {3-i}\theta^\prime(wx^{(0)}_i)x^{(0)}_i$.

\end{document}